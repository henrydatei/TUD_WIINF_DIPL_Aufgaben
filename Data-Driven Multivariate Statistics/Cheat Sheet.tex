\documentclass[10pt,landscape,a4paper]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape,top=1cm,bottom=1cm,right=1cm,left=1cm,noheadfoot,bindingoffset=0pt,marginparwidth=0pt,marginparsep=0pt]{geometry}
\usepackage{amsmath,amsfonts,amssymb,mathtools}
\usepackage{graphicx}
\usepackage{fontenc}
%\usepackage{lua-visual-debug}
\usepackage{enumitem}
\usepackage{xfrac}
\usepackage[utf8]{inputenc}

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {5pt}{1pt}%x
                                {\normalfont\small\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {5pt}%
                                {1pt}%
                                {\normalfont\small\underline}}
\makeatother

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\setlist{
	noitemsep,
	topsep=-\parskip,
	leftmargin=2em
}
\setcounter{secnumdepth}{0}

\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\SD}{SD}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Eigenvector}{Eigenvector}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols*}{4}


% multicol parameters
% These lengths are set only within the two main columns
\setlength{\columnseprule}{0.1pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{1pt}

\begin{center}
	\normalsize{\textbf{Graphical Representation of Numerical Data}} \\
\end{center}
\underline{Kernel density estimator:}
\begin{align}
	\hat{f}_h(x) &= \frac{1}{2nh}\sum_{i=1}^{n}I(\vert x-x_i\vert\le h) \quad\text{(Histogram)} \notag \\
	\Rightarrow\hat{f}_h(x) &= \frac{1}{nh}\sum_{i=1}^n K\left(\frac{x-x_i}{h}\right) \notag
\end{align}
with Kernel $K(u) = I(\vert u\vert \le 0.5)$ (uniform Kernel), $h$...Degree of smoothness

\begin{center}
	\normalsize{\textbf{Decision Trees}} \\
\end{center}
\underline{Impurity functions $u(R)$:} $p_k(R) = \frac{1}{n}\sum I(y_i=k)$ (purity)
\begin{itemize}
	\item classification error: $1-\max_k p_k(R)$
	\item Gini index: $1-\sum_k p_k(R)^2$
	\item entropy: $-\sum_k p_k(R)\cdot\log(p_k(R))$
	\item[$\Rightarrow$] are all maximized when $p_k$ is uniform on the $K$ classes in $R$; all are minimized when $p_k = 1$ for some $k$ ($R$ has one class)
\end{itemize}
\underline{Growing a classification tree:} $R\to R^+$ and $R^-$
\begin{itemize}
	\item calculate $u(R)$, $u(R^+)$, $u(R^-)$
	\item Gini improvement: $u(R) - (p(R^-)\cdot u(R^-) + p(R^+)\cdot u(R^+)) \to\max$
	\item[$\Rightarrow$] reduces uncertainty
\end{itemize}
\underline{Growing a regression tree:}
\begin{align}
	\hat{y} &= \sum_{m=1}^M c_mI(x\in R_m) \notag \\
	\Rightarrow \hat{c}(R_m) &= \frac{1}{n(R_m)}\sum_{i=1}^n I(y_i\mid x_i\in R_m) \notag
\end{align}
\underline{Stopping parameters:}
\begin{itemize}
	\item \#splits, \#observations per region, $\Delta$ objective function
	\item tree prunning: $R_\alpha(T) = \frac{1}{\sum (y_i-\bar{y})^2}\sum_{m=1}^{\vert T\vert}\sum_{i:x_i\in R_m} (y_i-f(x_i))^2 + \alpha\vert T\vert\to\min$
	\item $\alpha$...complexity parameter (CP)
\end{itemize}
\underline{Optimal subtree with CV:}
\begin{itemize}
	\item trees $T_0$ (0 splits), ..., $T_m$ ($m$ splits) $\Rightarrow$ $\infty,\alpha_1,...,\alpha_{min}$
	\item $\beta_i = \sqrt{\alpha_i \cdot\alpha_{i+1}}$ (average)
	\item subsets $G_1,...,G_B$
	\item trees with $\beta_1,...,\beta_m$ for each subset (leave one out $\to$ forecast)
	\item[$\Rightarrow$] smallest $\beta$ over all $G_i$'s
\end{itemize}
\underline{Bagging:} bootstrapping from training data, tree $T_i$
\begin{itemize}
	\item prediction: $\frac{1}{n}\sum_{i=1}^n \text{pred}(T_i,x)$
	\item when correlation in bootstrap samples $\to$ effect decreases
\end{itemize}
\underline{Random Forests:} bootstrapping + subset of explanatory variables
\begin{itemize}
	\item importance of variable: how much increase of MSE or classification error when variable is permuted in left-out-sample
\end{itemize}
\underline{Boosting:} AdaBoost: 
\begin{itemize}
	\item bootstrapping from training data with distribution $w_t$ ($w_0 = \frac{1}{n}$)
	\item train classifier $f_t$
	\item $\varepsilon_t = \sum w_t\cdot I(y_t\neq f_t(x_i))$, $\alpha_t = \frac{1}{2}\log\left(\frac{1-\varepsilon_t}{\varepsilon_t}\right)$
	\item scale $w_{t+1}(i) = w_t(i)\cdot\exp(-\alpha_ty_if(x_i))$ and normalize
	\item training error $\frac{1}{n}\sum I(y_i\neq f_{boost}(x_i)) \le \exp\left(-2\sum \left(\frac{1}{2}-\varepsilon_t\right)^2\right)$
	\item[$\Rightarrow$] $f_{boost} = \sgn\left(\sum \alpha_tf_t\right)$
\end{itemize}
\underline{CHAID:} not binary, split so that ANOVA has smallest $p$-value

\begin{center}
	\normalsize{\textbf{Nearest Neighbors Classifiers}} \\
\end{center}
\underline{NN classifier:} label $x$ with label of closest point (default euclidean distance)

\underline{$k$-NN classifier:} majority vote of $k$ closest points

\begin{center}
	\normalsize{\textbf{Linear classifier and Perceptron}} \\
\end{center}
\underline{Hyperplane $H$:} $p-1$ dimensional subspace $H=\{x\mid \langle x,w\rangle = 0\}$
\begin{itemize}
	\item affine Hyperplane $H = \{x\mid \langle x,w\rangle + w_0 = 0\}$
	\item[$\Rightarrow$] linear classifier: $\sgn(\langle x,w\rangle + w_0)$
\end{itemize}
\underline{Perceptron:}
\begin{itemize}
	\item $L=-\sum (y_i\cdot\langle x_i,w\rangle)I(y_i\neq\sgn(\langle x,w\rangle))\to\min$
	\item $\frac{\partial L}{\partial w}$ direction in which $L$ is increasing $\Rightarrow$ $w'=w-\eta\frac{\partial L}{\partial w}$ (gradient descent, Perceptron uses a stochastic version): find one $(x_iy_i)$ where $y_i\neq\sgn(\langle x_i,w\rangle)$ and then $w_{t+1}=w_t + \eta y_ix_i$
	\item problems: data separable $\to$ one $H$ will be found (not necessary the best), data not separable $\to$ algorithm won't stop
\end{itemize}

\begin{center}
	\normalsize{\textbf{Maximum Margin Classifiers and SVM}} \\
\end{center}
\underline{Maximum margin classifier:}
\begin{itemize}
	\item margin: distance $H \leftrightarrow$ closest point $\to\max$
	\item convex hull: every point can be reached by linear combination of convex-hull-points
	\item How to find $H$?
	\item[$\to$] $\langle x_i,w\rangle + w_0\ge 1$ ($y_i=1$), $\langle x_i,w\rangle + w_0\le -1$ ($y_i=-1$) $\Rightarrow$ $y_i(\langle x_i,w\rangle + w_0)-1\ge 0$
	\item[$\to$] $d_+ = d_- = \frac{1}{\Vert w\Vert} \Rightarrow d_++d_-=\frac{2}{\Vert w\Vert}\to\max$
	\item[$\to$] primal problem: $L=\frac{1}{2}\Vert w\Vert^2\to\min$ s.t. $y_i(\langle x_i,w\rangle + w_0)\ge 1$ (with Lagrange coefficients $\alpha$)
	\item[$\to$] dual problem $L=\sum\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j\langle x_i,x_j\rangle\to\max$ s.t. $\sum\alpha_iy_i = 0$ (find closest points in convex hull)
	\item[$\to$] solve dual problem for $\alpha$, $w=\sum\alpha_iy_ix_i$, pick $i$ for $\alpha_i>0$ and solve $y_i(\langle x_i,w\rangle + w_0)\ge 1$ for $w_0$
	\item[$\to$] problems: robustness, if not linear separable $\to$ not working
\end{itemize}
\underline{Support Vector Classifier:}
\begin{itemize}
	\item slack variables $\xi_i$ represent violation from strict separation, $y_i(\langle x_i,w\rangle + w_0)\ge 1-\xi_i$
	\item $L=\frac{1}{2}\Vert w\Vert^2 + \lambda\sum \xi_i$ s.t. $y_i(...)\ge 1-\xi_i$
	\item problems: non separability
\end{itemize}
\underline{Support Vector Machine:} maps data in higher dimensions with $\phi(x)$
\begin{itemize}
	\item primal problem: SVC with $x\to \phi(x)$
	\item dual problem: MMC with $x\to \phi(x)$ s.t. $0\le\alpha_i\le\lambda$, $\sum\alpha_iy_i = 0$
	\item[$\Rightarrow$] if $K$ exists such that $K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle$ we can use SVM without knowing $\phi$ (kernel trick)
	\item $K(x_i,x_j)$ pos. def. $\Leftrightarrow$ $\sum\sum \lambda_i\lambda_j K(x_i,x_j) > 0$
	\begin{itemize}
		\item linear kernel ($\langle x_i,x_j\rangle$)
		\item polynomial kernel ($(\langle x_i,x_j\rangle + 1)^p$)
		\item radial kernel ($\exp(-\Vert x_i-x_j\Vert/2\sigma^2)$)
		\item sigmoid kernel ($\tanh(k\langle x_i,x_j\rangle - \delta)$)
	\end{itemize}
	\item SVM with $K$ classes: 
	\begin{itemize}
		\item 1-vs-1: $\binom{K}{2}$ pairs, $\binom{K}{2}$ classifiers $\Rightarrow$ majority vote
		\item 1-vs-all: compare one of $K$ classes to rest, assign $x^\ast$ to $b_k + w_{1k}x_1^\ast + ...\to\max$
	\end{itemize}
\end{itemize}

\begin{center}
	\normalsize{\textbf{K-Means}} \\
\end{center}
\underline{K-Means:}
\begin{itemize}
	\item objective function: $\hat{\mu},\hat{c} = \arg\min L = \sum_k\sum_{i:c_i=k} \Vert x_i-\mu_k\Vert^2$
	\item non convex objective function (no global optimum findable, no derivatives, no gradient descent)
	\item update classes, update centers until nothing changes (start with random centers) $\to$ run multiple times
\end{itemize}
\underline{K-Means in compression:} similar colors $\to$ same color (cluster)

\underline{choose $K$:} advanced knowledge, increasing $K$ leads to more relative reduction of $L$ when $K$ is to small than $K$ is to big

\underline{Extensions:}
\begin{itemize}
	\item K-mediods: use $L_1$ norm instead of $L_2$ norm $\to$ more robust to outliers
	\item weighted K-means: $L=\sum_i\sum_k \phi_i(k)\frac{\Vert x_i-\mu_k\Vert^2}{\beta}$ where $\phi_i(k)>0$ and $\sum_k \phi_i(k)=1$, $\beta>0$
	\begin{align}
		\phi_i(k) &= \frac{\exp\left(-\frac{1}{\beta}\Vert x_i-\mu_k\Vert^2\right)}{\sum_k \exp\left(-\frac{1}{\beta}\Vert x_i-\mu_k\Vert^2\right)} \notag \\
		\mu_k &= \frac{\sum_i x_i\phi_i(k)}{\sum_i \phi(k)} \notag
	\end{align}
\end{itemize}
\underline{Generalized mixture model building:} EM-Algorithm
\begin{itemize}
	\item each cluster contains points from normal distribution with $\mu_i$, $\Sigma_i$ (different sizes of clusters possible)
	\item E-Step: update $\phi_i(k)$ (to which of the $K$ normal distributions a point belongs)
	\item M-Step: update parameters of each normal distribution
\end{itemize}

\begin{center}
	\normalsize{\textbf{Cluster Analysis}} \\
\end{center}
\underline{Proximity for non-metric data, distances for metric data}
\begin{itemize}
	\item binary data: $\frac{a_1+\delta a_4}{a_1 + \delta a_4 + \lambda(a_2+a_3)}$ $\to$ matching coefficient $\lambda = \delta = 1$
	\item mixed scales:
	\begin{itemize}
		\item nominal/ordinal: $d_{ij}^{(k)} = I(x_{ik} \neq x_{jk})$
		\item metric: $d_{ij}^{(k)} = \frac{\vert x_{ik} - x_{jk}\vert}{\max x_{mk} - \min x_{mk}}$
		\item $\delta_{ij}^{k} = 0$ if missing, else 1
		\item[$\Rightarrow$] Gower coefficient: $d_{ij} = \frac{\sum w_k\delta_{ij}^{(k)}d_{ij}^{(k)}}{\sum w_k\delta_{ij}^{(k)}}$
	\end{itemize}
	\item $L_p$ norm
	\item French railway metric: over Paris
	\item Karlsruhe metric: along arcs
	\item Mahalanobis distance: $d_{ij}^2 = (x_i-x_j)^\top A (x_i-x_j)$
	\item Contingency tables: $d^2(r_1,r_2) = \sum\left(\frac{x_{\cdot\cdot}}{x_{\cdot j}}\right)\left(\frac{x_{r_1j}}{x_{r_1\cdot}} - \frac{x_{r_2j}}{x_{r_2\cdot}}\right)^2$
	\item $Q$-Correlation distance: correlation between $x_i$ and $x_j$ in $k$-th variable
\end{itemize}
\underline{Distance between clusters:}
\begin{itemize}
	\item single linkage: nearest points $\to$ large groups
	\item complete linkage: farthest points
	\item average linkage: mean of all combinations
	\item centroid: $d(R, \text{center of gravity}(P+Q))$
	\item Ward: join groups that not increase heterogeneity to much ($I(R) = \frac{1}{n_R}\sum d^2(x_i,\bar{x}_R)$)
\end{itemize}
\underline{hierarchical:} joins/splits groups, \underline{partitioning:} exchange elements in given clustering

\begin{center}
	\normalsize{\textbf{Missing data}} \\
\end{center}
\underline{Types of missing data:}
\begin{itemize}
	\item missing completely at random
	\item missing at random: depends on observed data
	\item missing not at random: depends on observed predictors or on missing value itself
	\item[$\Rightarrow$] not testable!
\end{itemize}
\underline{What to do?}
\begin{itemize}
	\item deletion: assumes MCAR
	\item pairwise deletion: assumes MCAR ("merges" rows to get full data)
	\item unconditional location (cold deck): use value from "closest" observation $\to$ mean of $Y$ is wrong, variance of $Y$ is wrong
	\item unconditional mean: mean of all other observations $\to$ mean of $Y$ is good, variance of $Y$ is wrong
	\item unconditional distribution (hot deck): use randomly selected observation $\to$ mean/variance of $Y$ is good, $\Cor(X,Y)$ is wrong
	\item conditional mean (linear regression) $\to$ conditional mean of $Y$ is good, $\Cor(X,Y)$ is good, conditional variance of $Y$ is wrong
	\item conditional distribution (linear regression + $\varepsilon$) $\to$ conditional mean/variance of $Y$ is good, $\Cor(X,Y)$ is good
	\item random Forests: good if MAR
	\item time series: last observation carried forward, next observation carried backward
	\item time series: interpolation linear, seasonal interpolation
	\item[$\Rightarrow$] multiple imputations: impute, estimate parameter, ...
\end{itemize}

\begin{center}
	\normalsize{\textbf{Markov Chains}} \\
\end{center}
\begin{itemize}
	\item $M_{ij}$...probability of going from state $i$ to state $j$ $\Rightarrow$ row sums = 1
	\item estimate $M$: $\hat{M}_{ij} = \frac{\#\text{transitions } i\to j}{\#\text{transitions } i\to\ast}$
	\item $w_{t+1} = w_tM$, stationary $w_\infty = w_\infty M$ $\xrightarrow[\text{Eigenvalue}]{\lambda = 1} w_\infty = \frac{\gamma}{\Vert\gamma\Vert_1}$
	\item Markov Chains as ranking: A beats B, then B $\to$ A high and A $\to$ B low
	\item Markov Chains as classification: $\hat{M}_{ij} = \exp\left(-\frac{\Vert x_i-x_j\Vert^2}{b}\right)$ $\to$ normalize, if $x_i$ has label: $M_{ii} = 1$, rest of row 0 $\Rightarrow$ absorbing state
	\begin{align}
		M = \begin{pmatrix}
			A & B \\ 0 & I
		\end{pmatrix} \Rightarrow w_\infty &= w_0M^\infty \notag \\
		&= w_0\begin{pmatrix}
			0 & (I-A)^{-1}B \\ 0 & I
		\end{pmatrix} \notag
	\end{align}
	\item hidden Markov Model: hidden sequence of states, observation is drawn from distribution associated with state $\to$ EM-algorithm
\end{itemize}

\begin{center}
	\normalsize{\textbf{Neural Networks}} \\
\end{center}
\underline{Activation functions:}
\begin{align}
	f(I_j) &= \begin{cases}
		1 & I_j\ge\theta_j \\
		0 & \text{else}
	\end{cases} \notag \\
	f(I_j) &= \frac{1}{1+\exp(-\beta(I_j-\theta_j))} \quad\text{(sigmoid)} \notag \\
	f(I_j) &= \max(0,I_j) \quad\text{(relu)} \notag \\
	f(I_j) &= \log(1+\exp(I_j)) \quad\text{(solftplus)} \notag \\
	S(y_j) &= \frac{\exp(y_i)}{\sum\exp(y_i)} \quad\text{(softmax)} \notag
\end{align}
\underline{Back-propagation:} update weights with gradient decent

\underline{Reinforcement learning:} find best policy to max rewards $\to$ Q-learning: $Q(s,a) = r + \gamma\max_a Q(s',a)$

\end{multicols*}
\end{document}